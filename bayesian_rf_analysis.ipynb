{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "varying-rebound",
   "metadata": {},
   "source": [
    "# An application of Bayesian regression in visual neurosicence: modeling the relationship between receptive field size and eccentricity\n",
    "\n",
    "### by Hsin-Hao Yu\n",
    "Department of Physiology, Monash University\n",
    "\n",
    "**Background:**\n",
    "In studies of natural phenomena, if two measured quantities are observed to be in a certain relationship, very often we want to know if this relationship is affected by a certain manipulation. This is a basic form of statistical inferencing that has found many applications in scientific and clinical research.\n",
    "\n",
    "In a recent paper published in _The Journal of Neuroscience_ ([Hadjidimitrakis et al., 2019](https://www.jneurosci.org/content/39/27/5311.abstract)), I was asked to perform this type of analysis for a dataset collected in the visual cortex. I solved this problem with Model II regression and an ad hoc permutation test, and was able to establish that the power law that governed his data was dependent on the site where the data was collected. Although I got the job done, I was not fully satisfied with it, because all that it did was producing a p-value. I’d be happier if all the questions that neuroscientists want to know about in this dataset could be answered within a unified framework.\n",
    "\n",
    "Soon after I finished this analysis, a biostatistician friend introduced me to JAGS and STAN. They are powerful and flexible packages that really make Bayesian inferencing accessible. As I was reading about them, I realized that the Bayesian approach is the right way to go for analyzing the power law of receptive field size in the visual cortex. Since this is a fundamental question in visual neuroscience, I decided to re-visit this dataset. A journal publication is being prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-uncertainty",
   "metadata": {},
   "source": [
    "**The scientific question:** In the visual cortex, the receptive field (RF) of a neuron is the region in the visual field where a stimulus can evoke the neuron to respond. Importantly, in the cortex of highly visual animals, the size of the RF is dependent on its distance to the fovea - a measurement called _eccentricity_. This relationship has profound implications in the overall architecture of the visual system. The visual cortex consists of many \"modules\" called _areas_. Comparing RF size across multiple areas provides important clues about their functions, and can inform the identification of visual areas. Unfortunately, a rigorous framework for this comparison has not been  established. With the recent popularity of measuring \"population RF\" in functional imaging, RF data are becoming abundant, which makes RF quantification and comparison a more pressing issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-tomato",
   "metadata": {},
   "source": [
    "**Objectives:**  The goal of this notebook is to show that Bayesian regression provides a unified framework for analyzing the relationship between RF size and eccentricity, and can be implemented easily with R and modern Bayesian packages such as [JAGS](http://mcmc-jags.sourceforge.net) and [STAN](https://mc-stan.org). \n",
    "\n",
    "More specifically, I will show how to perform the following analyses the Bayesian way:\n",
    "\n",
    "1. Estimate the parameters of the RF size/eccentricity relationship, while taking into account the uncertainty of eccentricity measures (Type II regression)\n",
    "2. Decide the most parsimonious form of the relationship using Bayesian model section, and\n",
    "3. Characterize the differences of this relationship in two (or more) areas, using the Bayesian form of ANCOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-termination",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "careful-night",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "✔ ggplot2 3.1.1     ✔ purrr   0.3.2\n",
      "✔ tibble  3.0.5     ✔ dplyr   1.0.3\n",
      "✔ tidyr   1.1.2     ✔ stringr 1.4.0\n",
      "✔ readr   1.3.1     ✔ forcats 0.4.0\n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "Loading required package: rjags\n",
      "Warning message:\n",
      "“package ‘rjags’ was built under R version 3.6.3”Loading required package: coda\n",
      "Warning message:\n",
      "“package ‘coda’ was built under R version 3.6.3”Linked to JAGS 4.3.0\n",
      "Loaded modules: basemod,bugs\n",
      "\n",
      "Attaching package: ‘R2jags’\n",
      "\n",
      "The following object is masked from ‘package:coda’:\n",
      "\n",
      "    traceplot\n",
      "\n",
      "\n",
      "Attaching package: ‘runjags’\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    extract\n",
      "\n",
      "Registered S3 method overwritten by 'broom.mixed':\n",
      "  method      from \n",
      "  tidy.gamlss broom\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)    # for data manipulation\n",
    "library(patchwork)    # for composing figures\n",
    "\n",
    "library(lmodel2)      # for model II regression\n",
    "library(R2jags)       # for Bayesian analysis via JAGS\n",
    "library(runjags)      # for Bayesian analysis via JAGS\n",
    "library(broom.mixed)  # for tidying MCMC results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-american",
   "metadata": {},
   "source": [
    "Tell Jupyter to make the figures smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consolidated-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=6, repr.plot.height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-cowboy",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n",
    "This is the data used in Figure 10 in Hadjidimitrakis et al. (2019), which provides RF size and eccentricity information for neurons recorded in two areas (area V2 and V6) of the macqaue visual cortex. The dataset divides V6 RFs into the upper and the lower visual fields (`tmp.data.V6UVF` and `tmp.data.V6LVF`), but for the purpose of this notebook, we'll pool them together\n",
    "\n",
    "The csv files don't have headers so I have to add the column names myself. Also add a new column to indicate the cortical area where the data were collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "skilled-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "read.data <- function (filename, area.name){\n",
    "    data <- read.csv(filename, header=FALSE)\n",
    "    names(data)<-c(\"ecc\",\"rfsize\")\n",
    "    data <- data %>% mutate(area=area.name)\n",
    "    return (data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legendary-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.data.V2 <- read.data(\"../2021-bayes/2019-kostas/data/V2.csv\", \"V2\")\n",
    "tmp.data.V6UVF <- read.data(\"../2021-bayes/2019-kostas/data/UVF_V6.csv\", \"V6+\")\n",
    "tmp.data.V6LVF <- read.data(\"../2021-bayes/2019-kostas/data/LVF_V3_V6.csv\", \"V6-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-banks",
   "metadata": {},
   "source": [
    "Combine the 3 individual data sets into one data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "concrete-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- rbind(tmp.data.V2, tmp.data.V6UVF, tmp.data.V6LVF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-diploma",
   "metadata": {},
   "source": [
    "We are going to ignore the difference between \"V6+\" and \"V6-\", so they are aggregated into a single label called \"V6\" in a new column. `dplyr`'s `mutat()` function is used to add a new column `aggr.area` based on `area`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satisfactory-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- data %>% mutate(aggr.area=ifelse(area==\"V2\", \"V2\", \"V6\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-natural",
   "metadata": {},
   "source": [
    "We'll work in log scale a lot, so we'll create new columns, taking the log of `rfsize` and `ecc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thermal-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- data %>% mutate(\n",
    "    log2.ecc    = log(ecc,2),\n",
    "    log2.rfsize = log(rfsize,2) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-kidney",
   "metadata": {},
   "source": [
    "Export the curated version of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-kernel",
   "metadata": {},
   "source": [
    "## Summarize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-dressing",
   "metadata": {},
   "source": [
    "Check the number of samples after data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp<-summary(as.factor(data$area))\n",
    "data.frame(area=names(tmp), n=as.numeric(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp<-summary(as.factor(data$aggr.area))\n",
    "data.frame(area=names(tmp), n=as.numeric(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-filter",
   "metadata": {},
   "source": [
    "## Observe and explore\n",
    "\n",
    "### First, we examine the data for the individual areas\n",
    "The convention is to plot the RF size vs. eccentricity relationship in log-log coordinates, because it makes the heavily-skewed data more Gaussian. However, plotting the data without the log transformation is more intuitive for experimental scientists. Therefore, I will plot the same data in two formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rf <- function(input.data, input.areas, input.legend.pos = \"none\") {\n",
    "    ggplot(\n",
    "        input.data %>% filter(aggr.area %in% input.areas), \n",
    "        aes(x=ecc, \n",
    "            y=rfsize, \n",
    "            color=aggr.area)) + \n",
    "    geom_point(size=0.25) +\n",
    "    labs(\n",
    "        x=\"eccentricity (°)\",\n",
    "        y=\"RF size (°)\") +\n",
    "    scale_colour_discrete(\n",
    "        limits = c('V2', 'V6')) +\n",
    "    theme_classic() +\n",
    "    theme(\n",
    "        legend.position=input.legend.pos,\n",
    "        aspect.ratio=1,\n",
    "    )\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.V2 <- \n",
    "    plot.rf(data, \"V2\") +\n",
    "    scale_x_continuous(\n",
    "        breaks=c(10,20,30,40,50,60,70,80)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0,10,20,30,40)) +\n",
    "    ggtitle(\"linear scale\")\n",
    "\n",
    "g.V2.log2 <- \n",
    "    plot.rf(data, \"V2\") +\n",
    "    coord_trans(y='log2', x='log2') +\n",
    "    scale_x_continuous(\n",
    "        breaks=c(1,2,4,8,16,32,64)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0.5,1,2,4,8,16,32)) +\n",
    "    ggtitle(\"log scale\")\n",
    "\n",
    "g.V2 + \n",
    "g.V2.log2 +\n",
    "plot_annotation(\n",
    "    title = \"V2 RF size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-debate",
   "metadata": {},
   "source": [
    "The figure above illustrates the RF sizes for area V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.V6 <- \n",
    "    plot.rf(data,\"V6\") +\n",
    "    scale_x_continuous(\n",
    "        breaks=c(10,20,30,40,50,60,70,80)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0,10,20,30,40)) +\n",
    "    ggtitle(\"linear scale\")\n",
    "\n",
    "g.V6.log2 <- \n",
    "    plot.rf(data,\"V6\") +\n",
    "    coord_trans(y='log2', x='log2') +\n",
    "    scale_x_continuous(\n",
    "        breaks=c(1,2,4,8,16,32,64)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0.5,1,2,4,8,16,32)) +\n",
    "    ggtitle(\"log scale\")\n",
    "\n",
    "g.V6 + \n",
    "g.V6.log2 +\n",
    "plot_annotation(\n",
    "    title = \"V6 RF size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-trash",
   "metadata": {},
   "source": [
    "The figure above is the distribution of RF for area V6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-blood",
   "metadata": {},
   "source": [
    "### Then, we examine the two areas together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-gazette",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g.V2.V6 <- \n",
    "    plot.rf(data, c(\"V2\", \"V6\")) +\n",
    "    scale_x_continuous(\n",
    "        breaks=c(10,20,30,40,50,60,70,80)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0,10,20,30,40)) +\n",
    "    ggtitle(\"linear scale\")\n",
    "\n",
    "g.V2.V6.log2 <- \n",
    "    plot.rf(data, c(\"V2\", \"V6\"), input.legend.pos=\"right\") +\n",
    "    coord_trans(y='log2', x='log2') +\n",
    "    scale_x_continuous(\n",
    "        breaks=c(1,2,4,8,16,32,64)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0.5,1,2,4,8,16,32)) +\n",
    "    ggtitle(\"log scale\") +\n",
    "    labs(color=\"area\")\n",
    "\n",
    "g.V2.V6 + \n",
    "g.V2.V6.log2 +\n",
    "plot_annotation(\n",
    "    title = \"V2/V6 RF size comparison\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-sample",
   "metadata": {},
   "source": [
    "### Finally, we establish the objectives of the analysis\n",
    "\n",
    "From the figure above, we observe that V6 neurons tend to have larger RFs than V2 neurons at the same eccentricity. Also, RF sizes appear to grow more rapidly with eccentricity in V6 compared to V2. The rest of this notebook will try to quantify these observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-artist",
   "metadata": {},
   "source": [
    "## Least-square regression for area V2\n",
    "\n",
    "The most common way to analyze this data is to use least-sqaure linear regression on the log transformed data. In other words, we assume that RF size grows exponentially with eccentricity. However, more complicated equations have also been used. For example, [Rosa et al. (1997)](https://pubmed.ncbi.nlm.nih.gov/9373013/) used second-order polynomial (ie. quadratic) regression the characterize the size of V2 RFs. The objective of this section is to decide if such complexity is warranted.\n",
    "\n",
    "Note that since the data is pooled from multiple animals, [mixed models](https://m-clark.github.io/mixed-models-with-R/) are more appropriate. However, because this this dataset does not record the experiments where the individual data points came from, I will leave this issue to a further study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-preliminary",
   "metadata": {},
   "source": [
    "### First, we build a linear and a quadratic model, and inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.V2<-data %>% filter(aggr.area==\"V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V2.linear<-lm(log2.rfsize ~ log2.ecc, data.V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-longitude",
   "metadata": {},
   "source": [
    "Note that polynomial regression in R uses orthogonal polynomials to reduce the correlation between `ecc` and `ecc^2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V2.poly<-lm(log2.rfsize ~ poly(log2.ecc, 2), data.V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model.V2.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model.V2.poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-michigan",
   "metadata": {},
   "source": [
    "Calculate the 95% prediction interval of a given model. At a fixed eccentricity, this interval is the range of RF size that we are 95% confident that the real value falls into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.df<-function(df, model, interval='prediction') {\n",
    "    x = seq(\n",
    "        min(df$log2.ecc),\n",
    "        max(df$log2.ecc),\n",
    "        length.out=50\n",
    "    )\n",
    "    x.df <- data.frame(log2.ecc=x)\n",
    "    p <- predict(model, x.df, interval=interval)\n",
    "    p <- cbind(x.df, p)\n",
    "    new.df <- data.frame(\n",
    "        ecc = 2.0^x,\n",
    "        predicted = 2.0^(p$fit),\n",
    "        predicted.lwr = 2.0^(p$lwr),\n",
    "        predicted.upr = 2.0^(p$upr),\n",
    "        rfsize = 2.0^(p$fit),   # to fool ggplot\n",
    "        aggr.area = \"V2\"        # to fool ggplot\n",
    "    )\n",
    "    return(new.df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V2.linear.pred <- prediction.df(data.V2, model.V2.linear)\n",
    "model.V2.poly.pred <- prediction.df(data.V2, model.V2.poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pred <- \n",
    "    g.V2 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        color='black') +\n",
    "    geom_line(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        color='black',\n",
    "        linetype='dashed')\n",
    "\n",
    "g.pred.log2 <- \n",
    "    g.V2.log2 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        color='black') +\n",
    "    geom_line(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        color='black',\n",
    "        linetype='dashed')\n",
    "\n",
    "g.pred + \n",
    "g.pred.log2 + \n",
    "plot_annotation(\n",
    "    title = \"V2 RF size fitting\",\n",
    "    caption = \"Solid lines represents the linear model (in log-log space). Dashed line represents the quadratic model in log-log space \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-listing",
   "metadata": {},
   "source": [
    "The figure above shows that the linear model has a tendency to underestimate RF size at the very low end and the very high end of eccentricity. As pointed out previously by [Rosa et al. (1997)](https://pubmed.ncbi.nlm.nih.gov/9373013/), the quadratic model appears to capture the two ends more accurately. However, it is not clear at this point if the quadratic model is worthy of the trouble. The R^2 was only slighly improved, and there are good reasons to believe that deviations from the linear model are due to experimental errors. Measuring very small and very large RFs is technically difficult. They are easy to overestimate.\n",
    "\n",
    "Before we answer this question, let's examine the models a little further. Let's check the  95% prediction intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pred <-\n",
    "    g.V2 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black') +\n",
    "    geom_ribbon(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(ymin=predicted.lwr, ymax=predicted.upr),\n",
    "        color='black',\n",
    "        size=0.25,\n",
    "        alpha=0.2)\n",
    "\n",
    "g.pred.log2 <-\n",
    "    g.V2.log2 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black') +\n",
    "    geom_ribbon(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(ymin=predicted.lwr, ymax=predicted.upr),\n",
    "        color='black',\n",
    "        size=0.25,\n",
    "        alpha=0.2)\n",
    "\n",
    "g.pred + \n",
    "g.pred.log2 + \n",
    "plot_annotation(\n",
    "    title = \"V2 RF size fitting - linear model, 95% prediction interval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-modern",
   "metadata": {},
   "source": [
    "The figure above shows that the majority of the data points fall into the 95% confident range of the predicted values. There were only small number of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pred <-\n",
    "    g.V2 +\n",
    "    geom_line(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black',\n",
    "        linetype='dashed') +\n",
    "    geom_ribbon(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(ymin=predicted.lwr, ymax=predicted.upr),\n",
    "        color='black',\n",
    "        size=0.25,\n",
    "        alpha=0.2,\n",
    "        linetype='dashed')\n",
    "\n",
    "g.pred.log2 <-\n",
    "    g.V2.log2 +\n",
    "    geom_line(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black',\n",
    "        linetype='dashed') +\n",
    "    geom_ribbon(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(ymin=predicted.lwr, ymax=predicted.upr),\n",
    "        color='black',\n",
    "        size=0.25,\n",
    "        alpha=0.2,\n",
    "        linetype='dashed')\n",
    "\n",
    "g.pred + \n",
    "g.pred.log2 + \n",
    "plot_annotation(\n",
    "    title = \"V2 RF size fitting - quadratic model, 95% prediction interval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-startup",
   "metadata": {},
   "source": [
    "The figure above shows that the prediction interval of the quadratic model has a different shape, but it is not obviously better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-sweden",
   "metadata": {},
   "source": [
    "### Is the more complicated model justified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-mouth",
   "metadata": {},
   "source": [
    "Calculates the absolute errors of the model predictions, and translate back to the physical unit of degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs.errs <- function(df, model) {\n",
    "    tmp <- cbind(df, fitted(model))\n",
    "    n <- length(names(tmp))\n",
    "    names(tmp)[n] <- \"fitted\" # rename the last column to make it easier to refer to\n",
    "    errs <- abs(2.0^tmp$fitted - tmp$rfsize)\n",
    "    return (errs)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-thickness",
   "metadata": {},
   "source": [
    "The structure of this data frame is intended to be used for ggplot's boxplot, but I have omitted this plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs.df <- rbind(\n",
    "    data.frame(\n",
    "        model = rep('linear', nrow(data.V2)),\n",
    "        err = abs.errs(data.V2, model.V2.linear)),\n",
    "    data.frame(\n",
    "        model = rep('poly', nrow(data.V2)),\n",
    "        err = abs.errs(data.V2, model.V2.poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp <- summary(\n",
    "    cbind(\n",
    "        errs.df %>% filter(model==\"linear\") %>% select(err),\n",
    "        errs.df %>% filter(model==\"poly\") %>% select(err)))\n",
    "\n",
    "attr(tmp, \"dimnames\")[2] <- list(c(\"  err (linear)\", \"  err (quadratic)\"))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-dispute",
   "metadata": {},
   "source": [
    "The table above summarizes the absolute errors of the linear model (left) and the quadratic model (right), in the physical unit of RF size (°). The errors of the second model are smaller, but the differences are small (less than 1°). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-department",
   "metadata": {},
   "source": [
    "More complex models have higher degrees of freedom, so it's not surprising that their predictions are more accurate. They are not necessarily better models. The crucial question is if the performance gained is justified by the higher number of parameters. A Bayesian approach for selecting the better model begins with the calculation of the Bayesian Information Criterion (BIC): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "bic0 <- BIC(model.V2.linear)\n",
    "bic1 <- BIC(model.V2.poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-demand",
   "metadata": {},
   "source": [
    "Assuming that the linear and the quadratic model have the same prior probability, the model with the smaller BIC is the better model. We can conclcude that the quadratic model is indeed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(model = c('linear','quadratic'), BIC=c(bic0, bic1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-alcohol",
   "metadata": {},
   "source": [
    "The following is the posterior probability that the quadratic model is the better model of the two. Based on this probability, we conclude that the use of second-order polynomials is justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp(-0.5*bic1)/(exp(-0.5*bic0) + exp(-0.5*bic1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-debut",
   "metadata": {},
   "source": [
    "## Use Model II regression to account for the errors of eccentricity measures\n",
    "In least-square regression, the covariate is not a random variable. It is typically a factor that is manipulated in experiments so it is not subject to measurement error. However, in our problem, eccentricity is a measured quantity. The uncertainty associated with it is not accounted for in the least-square model. \n",
    "\n",
    "Model II regression can be used to address this issue. In R, we can use the [lmodel2](https://cran.r-project.org/web/packages/lmodel2/lmodel2.pdf) package. The package implements several estimation methods. Since the magnitude of error of eccentricity should be similar to that of RF size, we will use major-axis regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V2.linear2 <- lmodel2(log2.rfsize ~ log2.ecc, data = data.V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-waterproof",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.V2.linear2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-group",
   "metadata": {},
   "source": [
    "This function extracts the major axis model from the model II regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "get.MA.func <- function(model) {\n",
    "    params <- model$regression.results %>% filter(Method==\"MA\")\n",
    "    intercept <- params$Intercept\n",
    "    slope <- params$Slope\n",
    "    f <- function(log2.ecc) {\n",
    "        return (intercept + slope*log2.ecc)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2.df<-function(df, f) {\n",
    "    x = seq(\n",
    "        min(df$log2.ecc),\n",
    "        max(df$log2.ecc),\n",
    "        length.out=50\n",
    "    )\n",
    "    new.df <- data.frame(\n",
    "        log2.ecc = x,\n",
    "        ecc = 2.0^x,\n",
    "        aggr.area = \"V2\"\n",
    "    )\n",
    "    new.df <- new.df %>% mutate(\n",
    "        predicted=2.0^f(log2.ecc),\n",
    "        rfsize=2.0^f(log2.ecc)\n",
    "    )\n",
    "    return(new.df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V2.linear2.pred <- prediction2.df(data.V2, get.MA.func(model.V2.linear2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pred <-\n",
    "    g.V2 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black') +\n",
    "    geom_line(\n",
    "        data=model.V2.linear2.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black',\n",
    "        linetype='dashed')\n",
    "\n",
    "g.pred.log2 <-\n",
    "    g.V2.log2 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black') +\n",
    "    geom_line(\n",
    "        data=model.V2.linear2.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black',\n",
    "        linetype='dashed')\n",
    "\n",
    "g.pred + \n",
    "g.pred.log2 +\n",
    "plot_annotation(\n",
    "    title = \"V2 RF size Model II fitting\",\n",
    "    caption = \"Solid lines represents the least-square linear model. Dashed line represents the result of Model II regression \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-kingdom",
   "metadata": {},
   "source": [
    "Recall that at very large eccentricity, least-square regression (solid lines in the figure above) tends to underestimate the RF size, but this error can be reduced by the quadratic model. The figure above suggests that this underestimation might be due to the uncertainty of eccentricity. \n",
    "\n",
    "Interestingly, Model II regression estimated even smaller RFs at the lowest range of eccentricity than the least-square estimates. This result is consistent with the common finding that least-square regression tends to underestimate the slopes of measured covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-catalyst",
   "metadata": {},
   "source": [
    "## The analysis of V6 RFs\n",
    "\n",
    "Now, we perform the same analysis for area V6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.V6<-data %>% filter(aggr.area==\"V6\")\n",
    "model.V6.linear<-lm(log2.rfsize ~ log2.ecc, data.V6)\n",
    "model.V6.poly<-lm(log2.rfsize ~ poly(log2.ecc, 2), data.V6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "bic0 <- BIC(model.V6.linear)\n",
    "bic1 <- BIC(model.V6.poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp(-0.5*bic1)/(exp(-0.5*bic0) + exp(-0.5*bic1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V6.linear2 <- lmodel2(log2.rfsize~log2.ecc, data.V6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V6.poly.pred <- prediction.df(data.V6, model.V6.poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pred <-\n",
    "    g.V2.V6 +\n",
    "    geom_line(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='red') +\n",
    "    geom_line(\n",
    "        data=model.V6.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='blue') +\n",
    "    theme(\n",
    "        aspect.ratio=1,\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank(),\n",
    "        axis.line = element_line(\n",
    "            size = 0.4, \n",
    "            linetype = \"solid\",\n",
    "            colour = \"black\")\n",
    "    )\n",
    "\n",
    "g.pred.log2 <-\n",
    "    g.V2.V6.log2 +\n",
    "    geom_line(\n",
    "        data=model.V2.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='red') +\n",
    "    geom_line(\n",
    "        data=model.V6.poly.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='blue') +\n",
    "    theme(\n",
    "        aspect.ratio=1,\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank(),\n",
    "        axis.line = element_line(\n",
    "            size = 0.4, \n",
    "            linetype = \"solid\",\n",
    "            colour = \"black\")\n",
    "    )\n",
    "\n",
    "g.pred + \n",
    "g.pred.log2 +\n",
    "plot_annotation(\n",
    "    title = \"Quadratic, least-sqaure fitting of V2 and V6 RFs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V6.linear2 <- lmodel2(log2.rfsize~log2.ecc, data.V6)\n",
    "model.V6.linear2.pred <- prediction2.df(data.V6, get.MA.func(model.V6.linear2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pred <-\n",
    "    g.V2.V6 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear2.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='red') +\n",
    "    geom_line(\n",
    "        data=model.V6.linear2.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='blue')\n",
    "\n",
    "g.pred.log2 <-\n",
    "    g.V2.V6.log2 +\n",
    "    geom_line(\n",
    "        data=model.V2.linear2.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='red') +\n",
    "    geom_line(\n",
    "        data=model.V6.linear2.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='blue')\n",
    "\n",
    "g.pred + \n",
    "g.pred.log2 +\n",
    "plot_annotation(\n",
    "    title = \"Model II fittings of V2 and V6 RFs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-printing",
   "metadata": {},
   "source": [
    "## Use ANCOVA and permutation tests to decide if V2 and V6 are different "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-creation",
   "metadata": {},
   "source": [
    "The previous section strongly suggests that RFs are larger in V6 than in V2, but we have not established this observation statistically.\n",
    "\n",
    "This question can be formulated as a ANCOVA (Analysis of Covariance) problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "data$aggr.area <- as.factor(data$aggr.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm(log2.rfsize ~ log2.ecc * aggr.area, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-creator",
   "metadata": {},
   "source": [
    "The above indicates that the regression line is significantly dependent on the area (see the 4th row of `Coefficients`). We can also perform the same test for a quadratic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm(log2.rfsize ~ poly(log2.ecc, 2) * aggr.area, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-median",
   "metadata": {},
   "source": [
    "Again, the best-fit model is significantly dependent on the area. \n",
    "\n",
    "We have reached a statistical conclusion about RFs in V2 and V6. However, there are two problems:\n",
    "\n",
    "1. In ANCOVA, the slope in the linear model is assumed to be the same for the two areas. This assumption is  unlikely to be true, based on our knowledge about visual areas. The quadratic model also has the same problem.\n",
    "2. Since ANCOVA is a least-square method, it does not account for the uncertainty in eccentricity measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-deviation",
   "metadata": {},
   "source": [
    "To address these two issues, I implemented a permitation test in Hadjidimitrakis et al. (2019). I will not repeat it here, but in short:\n",
    "1. Under the null hypothesis that the RF size vs. eccentricity relationship is the same for V2 and V6, I took random samples from the pooled dataset, and used Model II regression to extract two model parameters (intercept and slope).\n",
    "2. I repeated this process many times, to build a joint distribution of the two parameters. \n",
    "3. I approximated the distribution with a bivariate Gaussian distribution, and used it to estimate the probability that the RF sizes in the two areas were indistinguishable, given the data.\n",
    "4. I concluded that the null hypothesis was unlikely (p<0.001)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-grass",
   "metadata": {},
   "source": [
    "This strategy did manage to establish a p-value for comparing two Model II regression results, but it was still unsatisfying for two reasons:\n",
    "1. The permutation test doesn't give the best estimate of the relationship, and\n",
    "2. It doesn't easily extend to polynomial models. Estimating the joing probability of the parameters would be difficult.\n",
    "\n",
    "In the rest of this notebook, we'll see if we can do better with Bayesian methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-notebook",
   "metadata": {},
   "source": [
    "## Simple Bayesian regression for area V2\n",
    "\n",
    "We start with the simplest Bayesian model. The log transformed RF size $y$ is related to the log transformed eccentricity $x$ as:\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "y \\sim N(\\mu_x, \\sigma^2) \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "\\mu_x = \\beta_0  + \\beta_1 x \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\beta_0$, $\\beta_1$ and $\\sigma$ are random variable with broadly distributed priors. Note that in this formulation, $y$ is a Gaussian random variable, but $x$ is not. This model is similar to the least-square model in that both do not take the uncertainty of eccentricity into account.\n",
    "\n",
    "This is how we implement it in JAGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle data for JAGS\n",
    "jags.Data.Desc <- list(\n",
    "    x = data.V2$log2.ecc,\n",
    "    y = data.V2$log2.rfsize\n",
    ")\n",
    "\n",
    "# define the Bayesian model\n",
    "jags.Model.String <- \"\n",
    "# standardize x and y into zx and zy\n",
    "data {\n",
    "    N <- length(y)\n",
    "    mx <- mean(x)\n",
    "    my <- mean(y)\n",
    "    sdx <- sd(x)\n",
    "    sdy <- sd(y)\n",
    "    for (i in 1:length(y)) {\n",
    "        zx[i] <- (x[i] - mx) / sdx\n",
    "        zy[i] <- (y[i] - my) / sdy\n",
    "    }\n",
    "}\n",
    "\n",
    "model {\n",
    "    # likelihood\n",
    "    for (i in 1:N) {\n",
    "        zy[i] ~ dnorm(mu[i], 1/zsigma^2)\n",
    "        mu[i] <- zbeta0 + zbeta1 * zx[i]\n",
    "    }\n",
    "    \n",
    "    # priors - these are all vague prior on standardized scale\n",
    "    zbeta0 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zbeta1 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zsigma ~ dunif( 1.0E-3, 1.0E+3)\n",
    "\n",
    "    # transform to original scale\n",
    "    beta1 <- zbeta1 * sdy / sdx\n",
    "    beta0 <- zbeta0 * sdy + my - zbeta1 * mx * sdy / sdx\n",
    "    sigma <- zsigma * sdy\n",
    "}\n",
    "\"\n",
    "writeLines(jags.Model.String, con=\"tmp-jags-model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-twist",
   "metadata": {},
   "source": [
    "Now use JAGS to run the MCMC simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "jags.V2.linear = jags(\n",
    "    data     = jags.Data.Desc,\n",
    "    inits    = NULL,\n",
    "    parameters.to.save = c('beta0', 'beta1','sigma'),\n",
    "    model.file = \"tmp-jags-model.txt\",\n",
    "    n.chains = 3,\n",
    "    n.iter   = 1000,\n",
    "    n.burnin = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-madison",
   "metadata": {},
   "source": [
    "Check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidyMCMC(as.mcmc(jags.V2.linear), conf.int = TRUE, conf.method = \"HPDinterval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-equality",
   "metadata": {},
   "source": [
    "The estimates are (unsurprisingly) very close to the estimates produced by least-square regression. While this table doesn't look too different from the least-square result, the meaning is very different. In the Bayesian model, the full posterior distributions of $\\beta_0$, $\\beta_1$ and $\\sigma$ have been estimated by Monte Carlo sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp <- as.matrix(as.mcmc(jags.V2.linear))\n",
    "\n",
    "g.tmp1a <- ggplot(\n",
    "    data.frame(tmp),\n",
    "    aes(x=beta0)) +\n",
    "geom_histogram(bins=30) + \n",
    "scale_x_continuous(\n",
    "    limits = c(-1.6,-1.0)\n",
    ") +\n",
    "theme_classic() +\n",
    "theme(\n",
    "    aspect.ratio=1)\n",
    "\n",
    "g.tmp2b <- ggplot(\n",
    "    data.frame(tmp),\n",
    "    aes(x=beta1)) +\n",
    "geom_histogram(bins=30) + \n",
    "scale_x_continuous(\n",
    "    limits = c(0.75, 0.95)\n",
    ") +\n",
    "theme_classic() +\n",
    "theme(\n",
    "    aspect.ratio=1)\n",
    "\n",
    "g.tmp3c <- ggplot(\n",
    "    data.frame(tmp),\n",
    "    aes(x=sigma)) +\n",
    "geom_histogram(bins=30) + \n",
    "scale_x_continuous(\n",
    "    limits = c(0.4, 0.75)\n",
    ") +\n",
    "theme_classic() +\n",
    "theme(\n",
    "    aspect.ratio=1)\n",
    "\n",
    "gg1 <-\n",
    "g.tmp1a + \n",
    "g.tmp2b + \n",
    "g.tmp3c \n",
    "\n",
    "gg1 +\n",
    "plot_annotation(\n",
    "    title = \"The posterior distributions of model parameters\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-bearing",
   "metadata": {},
   "source": [
    "In the figure above, the posterior distributin of $\\beta_1$ is of particular interest because it is the slope of the regression line. We can see that the estimated distribution is narrow. The 95% HPD (high posterior density) interval is (0.79, 0.88)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-arabic",
   "metadata": {},
   "source": [
    "Let's compare the Bayesian model to the least-square model. For this comparison, I'll only plot the uncertainty about $\\mu_x$ rather than $y$. Previosly, we plotted the prediction intervals of the least-square model. Here, we'll plot the confidence intervals, and then compare them to that of the Bayesian model. It's harder to work with the Bayesian model, because $\\beta_0$ and $\\beta_1$ are not just two numbers. Rather, they are sampled thousands of times by MCMC. We therefore have to calculate $\\mu_x$ thousands of times, and the calculate the HPD intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.newdf <- data.frame(\n",
    "    log2x = seq(\n",
    "        min(data.V2$log2.ecc), \n",
    "        max(data.V2$log2.ecc), \n",
    "        len = 200)\n",
    ")\n",
    "\n",
    "tmp.mcmc <- jags.V2.linear$BUGSoutput$sims.matrix\n",
    "tmp.X <- model.matrix(~log2x, tmp.newdf)\n",
    "tmp.coefs <- tmp.mcmc[, c(\"beta0\", \"beta1\")]\n",
    "tmp.fit <- tmp.coefs %*% t(tmp.X)\n",
    "tmp.newdf <- tmp.newdf %>% cbind(tidyMCMC(tmp.fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\n",
    "tmp.newdf <- tmp.newdf %>% mutate(x=2^log2x, y=2^estimate, y.low=2^conf.low, y.high=2^conf.high)\n",
    "\n",
    "tmp.g.bayes <- \n",
    "    ggplot(tmp.newdf, \n",
    "           aes(x = x, y = y)) + \n",
    "    geom_line(size=0.25) + \n",
    "    geom_ribbon(\n",
    "        aes(\n",
    "        ymin = y.low,\n",
    "        ymax = y.high), \n",
    "    fill = \"red\", alpha = 0.3) + \n",
    "    labs(\n",
    "        x=\"eccentricity (°)\",\n",
    "        y=\"RF size (°)\") + \n",
    "    theme_classic() +\n",
    "    theme(\n",
    "        aspect.ratio=1\n",
    "    ) +\n",
    "    ggtitle(\"bayesian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V2.linear.pred <- prediction.df(data.V2, model.V2.linear, interval = \"confidence\")\n",
    "tmp.g.leastsq <-\n",
    "    ggplot(model.V2.linear.pred, aes(x=ecc)) +\n",
    "    geom_line(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(x=ecc, y=predicted),\n",
    "        size=0.25,\n",
    "        color='black') +\n",
    "    geom_ribbon(\n",
    "        data=model.V2.linear.pred,\n",
    "        aes(ymin=predicted.lwr, ymax=predicted.upr),\n",
    "        fill='red',\n",
    "        alpha=0.3) +\n",
    "    labs(\n",
    "        x=\"eccentricity (°)\",\n",
    "        y=\"RF size (°)\") + \n",
    "    theme_classic() + \n",
    "    theme(\n",
    "        aspect.ratio=1\n",
    "    ) + \n",
    "    ggtitle(\"least-square\")\n",
    "\n",
    "tmp.g.leastsq +\n",
    "tmp.g.bayes + \n",
    "plot_annotation(\n",
    "    title = \"Comparison of the least-square and Bayesian models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-christian",
   "metadata": {},
   "source": [
    "The figure above shows that both models estimated similar levels of uncertainty for the mean RF size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-quantity",
   "metadata": {},
   "source": [
    "## Incorporating uncertainty about the covariate into the Bayesian model\n",
    "\n",
    "Previously, we used Model II regression to account for the uncertainty about the measured eccentricity. A problem is that because Model II regression is not a least-square method, it doesn't integrate well into the generalized linear model framework. Under the Bayesian framework, however, it is very natural to make $x$ a random variable. \n",
    "\n",
    "Note that in the implementation below, I gave RF size and eccentricity independent noises of the same magnitude. This reflects the fact that both measures were derived from the same RF outlines that were recorded in the experiment. Uncertainty about the two are expected to be similar. If the uncertainty of the two measures were allowed to be different, MCMC produced bi-stable estimates that are unrealistic in the context of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle data for JAGS\n",
    "jags.Data.Desc <- list(\n",
    "    x = data.V2$log2.ecc,\n",
    "    y = data.V2$log2.rfsize\n",
    ")\n",
    "\n",
    "# define the Bayesian model\n",
    "jags.Model.String <- \"\n",
    "# standardize x and y into zx and zy\n",
    "data {\n",
    "    N <- length(y)\n",
    "    mx <- mean(x)\n",
    "    my <- mean(y)\n",
    "    sdx <- sd(x)\n",
    "    sdy <- sd(y)\n",
    "    for (i in 1:length(y)) {\n",
    "        zx[i] <- (x[i] - mx) / sdx\n",
    "        zy[i] <- (y[i] - my) / sdy\n",
    "    }\n",
    "}\n",
    "\n",
    "model {\n",
    "    # likelihood\n",
    "    for (i in 1:N) {\n",
    "        zy[i]  ~ dnorm(mu[i], 1/zsigma^2)\n",
    "        zxn[i] ~ dnorm(zx[i], 1/zsigma^2)\n",
    "        mu[i] <- zbeta0 + zbeta1 * zxn[i]\n",
    "    }\n",
    "    \n",
    "    # priors - these are all vague prior on standardized scale\n",
    "    zbeta0 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zbeta1 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zsigma ~ dunif( 1.0E-3, 1.0E+3)\n",
    "\n",
    "    # transform to original scale. Algebra solved by Mathematica\n",
    "    beta1 <- zbeta1 * sdy / sdx\n",
    "    beta0 <- zbeta0 * sdy + my - zbeta1 * mx * sdy / sdx\n",
    "    sigma <- zsigma * sdy\n",
    "}\n",
    "\"\n",
    "writeLines(jags.Model.String, con=\"tmp-jags-model2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "jags.V2.linear2 = jags(\n",
    "    data     = jags.Data.Desc,\n",
    "    inits    = NULL,\n",
    "    parameters.to.save = c('beta0', 'beta1', 'sigma'),\n",
    "    model.file = \"tmp-jags-model2.txt\",\n",
    "    n.chains = 3,\n",
    "    n.iter   = 1000,\n",
    "    n.burnin = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidyMCMC(as.mcmc(jags.V2.linear2), conf.int = TRUE, conf.method = \"HPDinterval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp <- as.matrix(as.mcmc(jags.V2.linear2))\n",
    "\n",
    "g.tmp1a <- ggplot(\n",
    "    data.frame(tmp),\n",
    "    aes(x=beta0)) +\n",
    "geom_histogram(bins=30) + \n",
    "scale_x_continuous(\n",
    "    limits = c(-1.6,-1.0)\n",
    ") +\n",
    "theme_classic() +\n",
    "theme(\n",
    "    aspect.ratio=1)\n",
    "\n",
    "g.tmp2b <- ggplot(\n",
    "    data.frame(tmp),\n",
    "    aes(x=beta1)) +\n",
    "geom_histogram(bins=30) + \n",
    "scale_x_continuous(\n",
    "    limits = c(0.75, 0.95)\n",
    ") +\n",
    "theme_classic() +\n",
    "theme(\n",
    "    aspect.ratio=1)\n",
    "\n",
    "g.tmp3c <- ggplot(\n",
    "    data.frame(tmp),\n",
    "    aes(x=sigma)) +\n",
    "geom_histogram(bins=30) + \n",
    "scale_x_continuous(\n",
    "    limits = c(0.4, 0.75)\n",
    ") +\n",
    "theme_classic() +\n",
    "theme(\n",
    "    aspect.ratio=1)\n",
    "\n",
    "gg2 <- \n",
    "g.tmp1a + \n",
    "g.tmp2b + \n",
    "g.tmp3c \n",
    "\n",
    "\n",
    "gg1 /\n",
    "gg2 + \n",
    "plot_annotation(\n",
    "    title = \"The posterior distributions of model parameters\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-intervention",
   "metadata": {},
   "source": [
    "The figure above compares the posterior distributions of the parameters estimated for the simple Bayesian regression model that we have already seen in the previous section (top row), and those for the Bayesian model discussed in this section (bottom row). The regression lines (i.e., $\\beta_0$ and $\\beta_1$) are remarkably similar. The most significant difference is that by allowing errors in the covariate, we were able to derive a tighter relationship (i.e., $\\sigma$ is smaller).\n",
    "\n",
    "Recall that earlier in this notebook, we observed that frequentist Model II regression suggested that the slope of the regression line was underestimated by linear regression. The Bayesian model, however, does not agree with this conclusion. This is most likely because the noise structure of major-axis Model II regression is too flexible. It's an issue that requires further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-drunk",
   "metadata": {},
   "source": [
    "## Use Bayesian model section to evaluate the quadratic model\n",
    "\n",
    "As we have done previously, we want to know if RF size is better modeled as a quadratic function. Again we give both RF size and eccentricity some degree of uncertainty. This is a model that we couldn't do with `lmodel2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle data for JAGS\n",
    "jags.Data.Desc <- list(\n",
    "    x = data.V2$log2.ecc,\n",
    "    y = data.V2$log2.rfsize\n",
    ")\n",
    "\n",
    "# define the Bayesian model\n",
    "jags.Model.String <- \"\n",
    "# standardize x and y into zx and zy\n",
    "data {\n",
    "    N <- length(y)\n",
    "    mx <- mean(x)\n",
    "    my <- mean(y)\n",
    "    sdx <- sd(x)\n",
    "    sdy <- sd(y)\n",
    "    for (i in 1:length(y)) {\n",
    "        zx[i] <- (x[i] - mx) / sdx\n",
    "        zy[i] <- (y[i] - my) / sdy\n",
    "    }\n",
    "}\n",
    "\n",
    "model {\n",
    "    # likelihood\n",
    "    for (i in 1:N) {\n",
    "        zy[i]  ~ dnorm(mu[i], 1/zsigma^2)\n",
    "        zxn[i] ~ dnorm(zx[i], 1/zsigma^2)\n",
    "        mu[i] <- zbeta0 + zbeta1 * zxn[i] + zbeta2 * pow(zxn[i], 2)\n",
    "    }\n",
    "    \n",
    "    # priors - these are all vague prior on standardized scale\n",
    "    zbeta0 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zbeta1 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zbeta2 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zsigma ~ dunif( 1.0E-3, 1.0E+3)\n",
    "\n",
    "    # transform to original scale. Algebra solved by Mathematica\n",
    "    beta2 <- sdy * zbeta2 / pow(sdx, 2)\n",
    "    beta1 <- sdy * (sdx * zbeta1 - 2 * mx * zbeta2) / pow(sdx, 2)\n",
    "    beta0 <- my + sdy * (pow(sdx, 2) * zbeta0 - mx * sdx * zbeta1 + pow(mx, 2) * zbeta2)/pow(sdx, 2)\n",
    "    sigma <- zsigma * sdy\n",
    "}\n",
    "\"\n",
    "writeLines(jags.Model.String, con=\"tmp-jags-model3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "jags.V2.poly = jags(\n",
    "    data     = jags.Data.Desc,\n",
    "    inits    = NULL,\n",
    "    parameters.to.save = c('beta0', 'beta1', 'beta2', 'sigma'),\n",
    "    model.file = \"tmp-jags-model3.txt\",\n",
    "    n.chains = 3,\n",
    "    n.iter   = 1000,\n",
    "    n.burnin = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidyMCMC(as.mcmc(jags.V2.poly), conf.int = TRUE, conf.method = \"HPDinterval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-moderator",
   "metadata": {},
   "source": [
    "Note that the HPD interval of $\\beta_2$ excludes 0, so we have some evidence that the quadratic term does make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.newdf <- data.frame(\n",
    "    log2x = seq(\n",
    "        min(data.V2$log2.ecc), \n",
    "        max(data.V2$log2.ecc), \n",
    "        len = 200)\n",
    ")\n",
    "\n",
    "tmp.mcmc <- jags.V2.poly$BUGSoutput$sims.matrix\n",
    "#tmp.X <- model.matrix(~log2x, tmp.newdf)\n",
    "tmp.X <- as.matrix(data.frame(model.matrix(~log2x, tmp.newdf)) %>% mutate(log2x2=log2x^2))\n",
    "tmp.coefs <- tmp.mcmc[, c(\"beta0\", \"beta1\", \"beta2\")]\n",
    "tmp.fit <- tmp.coefs %*% t(tmp.X)\n",
    "tmp.newdf <- tmp.newdf %>% cbind(tidyMCMC(tmp.fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\n",
    "tmp.newdf <- tmp.newdf %>% mutate(x=2^log2x, y=2^estimate, y.low=2^conf.low, y.high=2^conf.high)\n",
    "\n",
    "tmp.newdf2 <- data.frame(\n",
    "    log2x = seq(\n",
    "        min(data.V2$log2.ecc), \n",
    "        max(data.V2$log2.ecc), \n",
    "        len = 200)\n",
    ")\n",
    "\n",
    "tmp.mcmc <- jags.V2.linear2$BUGSoutput$sims.matrix\n",
    "tmp.X <- model.matrix(~log2x, tmp.newdf)\n",
    "tmp.coefs <- tmp.mcmc[, c(\"beta0\", \"beta1\")]\n",
    "tmp.fit <- tmp.coefs %*% t(tmp.X)\n",
    "tmp.newdf2 <- tmp.newdf2 %>% cbind(tidyMCMC(tmp.fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\n",
    "tmp.newdf2 <- tmp.newdf2 %>% mutate(x=2^log2x, y=2^estimate, y.low=2^conf.low, y.high=2^conf.high)\n",
    "\n",
    "tmp.g.bayes1 <- \n",
    "    ggplot(tmp.newdf, \n",
    "           aes(x = x, y = y)) + \n",
    "    scale_x_continuous(\n",
    "        breaks=c(10,20,30,40,50,60,70,80)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0,10,20,30,40)) +\n",
    "    geom_line(\n",
    "        size=0.25, \n",
    "        linetype='dashed') + \n",
    "    geom_line(\n",
    "        data = tmp.newdf2,\n",
    "        aes(x = x, y = y),\n",
    "        size=0.25) + \n",
    "    geom_ribbon(\n",
    "        aes(\n",
    "        ymin = y.low,\n",
    "        ymax = y.high), \n",
    "    fill = \"red\", alpha = 0.3) + \n",
    "    labs(\n",
    "        x=\"eccentricity (°)\",\n",
    "        y=\"RF size (°)\") + \n",
    "    theme_classic() +\n",
    "    theme(\n",
    "        aspect.ratio=1\n",
    "    ) +\n",
    "    ggtitle(\"linear scale\")\n",
    "\n",
    "tmp.g.bayes2 <- \n",
    "    ggplot(tmp.newdf, \n",
    "           aes(x = x, y = y)) + \n",
    "    coord_trans(y='log2', x='log2') +\n",
    "    scale_x_continuous(\n",
    "        breaks=c(1,2,4,8,16,32,64)) +\n",
    "    scale_y_continuous(\n",
    "        breaks=c(0.5,1,2,4,8,16,32)) +\n",
    "    geom_line(\n",
    "        size=0.25,\n",
    "        linetype='dashed') + \n",
    "    geom_line(\n",
    "        data = tmp.newdf2,\n",
    "        aes(x = x, y = y),\n",
    "        size=0.25) + \n",
    "    geom_ribbon(\n",
    "        aes(\n",
    "        ymin = y.low,\n",
    "        ymax = y.high), \n",
    "    fill = \"red\", alpha = 0.3) + \n",
    "    labs(\n",
    "        x=\"eccentricity (°)\",\n",
    "        y=\"RF size (°)\") + \n",
    "    theme_classic() +\n",
    "    theme(\n",
    "        aspect.ratio=1\n",
    "    ) +\n",
    "    ggtitle(\"log scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.g.bayes1 + \n",
    "tmp.g.bayes2 + \n",
    "plot_annotation(\n",
    "    title = \"Comparing the Bayesian linear and quadratic models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-sending",
   "metadata": {},
   "source": [
    "The figure above shows that as in least-square regression, the quadratic model (dashed lines and pink bands) predicts larger RF at very high and very low eccentricities, compared to the linear model (solid line). I had suspected that once the uncertainty of eccentricity is taken into account, the quadratic term would become unnecessary. But that is not supported by Bayesian inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-parliament",
   "metadata": {},
   "source": [
    "Using a hierarchical Bayesian model, we can formally test if the quadratic model provides better explanation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-battery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "covered-merchant",
   "metadata": {},
   "source": [
    "## Bayesian analysis for area V6\n",
    "\n",
    "So have, we have a developed a good characterization of area V2. Let's do the same for area V6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle data for JAGS\n",
    "jags.Data.Desc <- list(\n",
    "    x = data.V6$log2.ecc,\n",
    "    y = data.V6$log2.rfsize\n",
    ")\n",
    "\n",
    "# define the Bayesian model\n",
    "jags.Model.String <- \"\n",
    "# standardize x and y into zx and zy\n",
    "data {\n",
    "    N <- length(y)\n",
    "    mx <- mean(x)\n",
    "    my <- mean(y)\n",
    "    sdx <- sd(x)\n",
    "    sdy <- sd(y)\n",
    "    for (i in 1:length(y)) {\n",
    "        zx[i] <- (x[i] - mx) / sdx\n",
    "        zy[i] <- (y[i] - my) / sdy\n",
    "    }\n",
    "}\n",
    "\n",
    "model {\n",
    "    # likelihood\n",
    "    for (i in 1:N) {\n",
    "        zy[i]  ~ dnorm(mu[i], 1/zsigma^2)\n",
    "        zxn[i] ~ dnorm(zx[i], 1/zsigma^2)\n",
    "        mu[i] <- zbeta0 + zbeta1 * zxn[i] + zbeta2 * pow(zxn[i], 2)\n",
    "    }\n",
    "    \n",
    "    # priors - these are all vague prior on standardized scale\n",
    "    zbeta0 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zbeta1 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zbeta2 ~ dnorm( 0, 1/(10)^2 )\n",
    "    zsigma ~ dunif( 1.0E-3, 1.0E+3)\n",
    "\n",
    "    # transform to original scale. Algebra solved by Mathematica\n",
    "    beta2 <- sdy * zbeta2 / pow(sdx, 2)\n",
    "    beta1 <- sdy * (sdx * zbeta1 - 2 * mx * zbeta2) / pow(sdx, 2)\n",
    "    beta0 <- my + sdy * (pow(sdx, 2) * zbeta0 - mx * sdx * zbeta1 + pow(mx, 2) * zbeta2)/pow(sdx, 2)\n",
    "    sigma <- zsigma * sdy\n",
    "}\n",
    "\"\n",
    "writeLines(jags.Model.String, con=\"tmp-jags-model5.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "jags.V6.poly = jags(\n",
    "    data     = jags.Data.Desc,\n",
    "    inits    = NULL,\n",
    "    parameters.to.save = c('beta0', 'beta1', 'beta2', 'sigma'),\n",
    "    model.file = \"tmp-jags-model5.txt\",\n",
    "    n.chains = 3,\n",
    "    n.iter   = 1000,\n",
    "    n.burnin = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidyMCMC(as.mcmc(jags.V6.poly), conf.int = TRUE, conf.method = \"HPDinterval\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
